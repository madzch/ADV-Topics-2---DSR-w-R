---
title             : "Proportional reasoning across formats"
shorttitle        : "PProportional reasoning across formats"
author: 
  - name          : "Madison Chin"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author



affiliation:
  - id            : "1"
    institution   : "Rutgers University"
  
keywords          : "keywords"
wordcount         : "X"

bibliography      : "r-references.bib"

floatsintext      : yes
linenumbers       : yes
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
library(tidyverse)
library(ggplot2)
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      tidy = FALSE,
                      knitr::opts_chunk$set(echo = FALSE,root.dir = '~/GitHub/ADV-Topics-2---DSR-w-R/WeeklyAssignments/Assignment 11'),
                      message = FALSE,
                      fig.align = 'center',
                      out.width = "100%")
options(knitr.table.format = "html")
library(tidyverse)
library(ggplot2)
library(ggdist)
prob_task <- read_csv("Chin_WA11_files/data/MFIndD_probtask.csv")
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

```{r themestyle}


```

# Introduction
Comparing proportions is sometimes very hard! But, even infants seem to be able to do it a little bit. The purpose of this science project was to better understand how well people compare proportions when the proportions are presented in different formats. The purpose of this class assignment is to take the R-code and plots weâ€™ve been generating over the last several weeks and put it all together into one poster format.

# Research Objectives: 
1.Does average performance vary across format type?
2.Does average performance vary across numerator congruency status?
3.Does numerator congruency vary across format type?(ie., is there an interaction)

## Participants
```{r numparts}
num <- 99

```
 A total of `r num ` adults participated in the study. 

# Methods
First participants were introduced to a story about a magic ball and that the outcome(ie., blue or orange) depended on the proportions. They were then asked to compare the proportions of different images.

In other words, participants were shown two images of the same kind at the same time and asked to decided which had a higher proportion of the shape (or dots) colored in blue.

```{r img1, fig.cap = " : An example of integrated blobs"}
knitr::include_graphics("img11/Probtask_Trial.png")

```
Conditions There were four different conditions that changed what kinds of images the participants saw:

- divided blobs: blur and orange were entirely separate.
- integrated blob: one blob, divided to be part blue and part orange.
- separated dots: blue and orange dots were on opposite sides of the image.
- integrated dots: blue and orange dots were intermixed.

```{r img2, fig.cap = " From left to right : divided blobs, integrated blobs, separated dots, integraed dots"}
knitr::include_graphics("img11/Probtask_formats.png")

```

\newpage

# Results

1. Does average performance vary across format type, ignoring all other aspects of the stimuli?


```{r dblcheck_prob_task, include = FALSE}
#Double checking summary is correct/Summarizing to get the proportion of the data correct. 
prob_data_summary <- prob_task%>%
  group_by(SubID, condition) %>%
  summarise(prop_corr = mean(correct), .groups = 'drop')  # Ensure ungrouping
print(prob_data_summary)
```

```{r recalcprop_corr, include = FALSE}
# Calculate mean proportion correct for each condition
mean_prop_summary <- prob_data_summary%>%
  group_by(condition) %>%
  summarise(mean_prop_corr = mean(prop_corr),
            sd_prop_corr = sd(prop_corr), prop_corr,  # Standard deviation for error bars(to help with readability of the plot)
            n = n(), .groups = 'drop')

```

```{r returnplot, include = TRUE, fig.cap = "Plot of Distribution of Accuracy by Condition" }
fixed_plot <- mean_prop_summary %>%
  mutate(condition_name = case_when(
    condition == "blob_shifted" ~ "Separate Blob", 
    condition == "blob_stacked" ~ "Half/Half Blob",
    condition == "dots_EqSizeSep" ~ "Separate Dots",
    condition == "dots_EqSizeRand" ~ "Mixed Dots"
  )) %>%
  ggplot(aes(x = condition_name, y = prop_corr )) +
  ggdist::stat_slab() +  # Adds a distributional layer to show the density of the data
  geom_point(aes(x = condition_name, y = mean_prop_corr), 
             color = "blue",position = position_nudge(y = 0.05)) +  # Offset mean points for clarity
  theme_minimal() +
  labs(title = "Distribution of Accuracy by Condition",
       x = "Format Type", y = "Accuracy") 
   #+theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Improved readability

fixed_plot + ylim(0,1)
```

*Yes, the blue dots in the above plot prove that the average performance varies across formatting types.*

`r fixed_plot`

2. 
\newpage

## Data analysis
We used `r cite_r("r-references.bib")` for all our analyses.


# Discussion


\newpage

# References
use r_refs( file = "references.bib")

::: {#refs custom-style="Bibliography"}
:::
